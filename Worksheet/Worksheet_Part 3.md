# DL workshop 2 -  Part 3

***

在了解CNN的内部结构之后，下面我们来通过一些常见的CNN神经网络，来了解如何搭建一个CNN模型。

## 1. LeNet

![图片](../images/CNN内部结构.png "CNN内部结构"  )

这张图片就是LeNet的结构图。它发布于上世纪90年代，是最早发布的卷积神经网络之一，用于识别手写数字。下面是它的具体连接方式：

![图片](../images/LeNet.png "LeNet结构图"  )

图中卷积层的括号中的数字表示输出通道数，全连接层的激活函数用的是sigmoid激活函数（因为当时更好用的ReLU函数还未出现）。代码见LeNet文件。

## 2.深度卷积神经网络

虽然LeNet在小数据集(MNIST)上取得了很好的效果，但是在更大、更真实的数据集上训练卷积神经网络的性能和可行性还有待研究。而2012年，AlexNet实现在GPU运行深度学习网络，证明了深度卷积神经网络的可行性。

### 2.1 AlexNet

AlexNet由八层组成，比LeNet的网络要深很多，并采用ReLU函数作为激活函数。AlexNet学习的数据集是ImageNet，图像的长、宽接近MNIST图像的10倍，因此卷积核，通道数都比LeNet要大。下面是简化版的AlexNet结构图（去掉GPU并行运算的部分）：

![图片](../images/AlexNet.png "AlexNet结构图"  )

因为ImageNet数据集太大，我们仅展示AlexNet的搭建代码，见AlexNet文件。

其中dropout是一种叫做**暂退法**的数据处理方法。在标准暂退法中，每个神经元的输出值h以暂退概率p由变量h′替换，如下所示：

![图片](../images/暂退法.png "暂退法"  )

输出被0替换的神经元可以看作从神经网络删除，这样减少了**过拟合**的风险。

### 2.2 使用块的CNN网络(VGG)

使用块的想法首先出现在牛津大学的视觉几何组的VGG网络中。通过使用循环和子程序，可以很容易地在任何现代深度学习框架的代码中实现这些重复的架构。

经典卷积神经网络的基本组成部分是下面的这个序列:

1. 带填充以保持分辨率的卷积层；
2. 非线性激活函数，如ReLU；
3. 汇聚层，如最大汇聚层。

而一个VGG块的结构与之类似，由**n个带有ReLU函数的卷积层，和1个最大汇聚层**组成，如下图所示：

![图片](../images/VGG块.png "VGG块"  )

一个VGG块的参数有三个：**卷积层个数，输入通道数，输出通道数**

通过把多个VGG块拼在一起，就可以得到一个VGG网络，结构如下：

![图片](../images/VGG网络.png "VGG网络"  )

代码见VGG文件。

### 2.3 其他使用块的CNN网络

除了VGG以外，还有其他CNN网络也是用堆叠块的思路搭成的，只是块内部构成和模块之间的连接方式不太一样，比如：

- NiN：使用NiN块，在块内部加入全连接层，并去掉网络最后的全连接层
- GoogleNet: 使用inception块，采用四条并行路径
- ResNet：使用残差块，将块的输入再次加入块的输出
![图片](../images/残差块.png "残差块"  )

### 参考资料

***

- [Dive into deep learning](https://d2l.ai/)
- [交大密院Deep Learning学习手册](https://github.com/Banyutong/deep_learning_hands_on)
  